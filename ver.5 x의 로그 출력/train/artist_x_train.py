# artist_x_secure_train.py
import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import BertModel, BertLMHeadModel, BertTokenizer, DataCollatorWithPadding
from datasets import load_dataset
from tqdm import tqdm
import warnings
import os
import random

warnings.filterwarnings("ignore")

# --- ArtistX ëª¨ë¸ í´ë˜ìŠ¤ ---
class ArtistX(nn.Module):
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.encoder = BertModel.from_pretrained(model_name)
        self.decoder = BertLMHeadModel.from_pretrained(model_name, is_decoder=True)
# ... (ì´í•˜ ëª¨ë¸ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)

# --- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == '__main__':
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MODEL_NAME = 'bert-base-uncased'
    LEARNING_RATE = 2e-5
    BATCH_SIZE = 16
    NUM_EPOCHS = 3 # ìµœì ì ì´ 3 ì—í¬í¬ ê·¼ì²˜ì˜€ìœ¼ë¯€ë¡œ, 3~5 ì •ë„ë¡œ ì„¤ì •

    # ğŸ’¡ --- ë³´ì•ˆ ê°•í™”: ê³ ìœ  ë°ì´í„°ì…‹ ìƒì„±ì„ ìœ„í•œ ë¹„ë°€ ì‹œë“œ --- ğŸ’¡
    # ì´ ì‹œë“œê°’ì„ ë³€ê²½í•˜ë©´ ì™„ì „íˆ ìƒˆë¡œìš´ ë§ˆìŠ¤í„° í‚¤ê°€ ìƒì„±ë©ë‹ˆë‹¤.
    # ì´ ê°’ì€ ë§ˆìŠ¤í„° í‚¤(.pth)ì™€ í•¨ê»˜ ë¹„ë°€ë¦¬ì— ë³´ê´€í•´ì•¼ í•©ë‹ˆë‹¤.
    SECRET_DATA_SEED = "Picasso-Protocol-Janus-2025" 
    DATA_SUBSET_RATIO = 0.95 # ì›ë³¸ ë°ì´í„°ì˜ 95%ë§Œ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.

    print(f"ğŸš€ ArtistX ë³´ì•ˆ ê°•í™” í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. Device: {DEVICE}")
    print(f"ë¹„ë°€ ì‹œë“œ: '{SECRET_DATA_SEED}'")

    # --- ë°ì´í„° ì¤€ë¹„ ---
    print("1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ê³ ìœ  ë³€í˜• ì‘ì—… ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    raw_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')
    
    # ğŸ’¡ --- ë°ì´í„°ì…‹ ë³€í˜• ë¡œì§ --- ğŸ’¡
    # 1. ë¹„ë°€ ì‹œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëœë¤ ìƒì„±ê¸° ì´ˆê¸°í™”
    rng = random.Random(SECRET_DATA_SEED)
    
    # 2. ì›ë³¸ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ì¸ë±ìŠ¤ë¥¼ ì„ìŒ
    train_indices = list(range(len(raw_dataset["train"])))
    rng.shuffle(train_indices)
    
    # 3. ì„ì¸ ì¸ë±ìŠ¤ì—ì„œ 95%ë§Œ ì„ íƒí•˜ì—¬ ê³ ìœ í•œ ì„œë¸Œì…‹ ìƒì„±
    num_samples_to_keep = int(len(train_indices) * DATA_SUBSET_RATIO)
    unique_train_indices = train_indices[:num_samples_to_keep]
    
    unique_train_dataset = raw_dataset["train"].select(unique_train_indices)
    print(f"ì›ë³¸ í›ˆë ¨ ë°ì´í„° {len(raw_dataset['train'])}ê°œ ì¤‘ {len(unique_train_dataset)}ê°œë¥¼ ì„ íƒí•˜ì—¬ ê³ ìœ  ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=128)

    # ğŸ’¡ ê³ ìœ í•˜ê²Œ ë³€í˜•ëœ í›ˆë ¨ ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì§•
    tokenized_train_dataset = unique_train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_val_dataset = raw_dataset["validation"].map(tokenize_function, batched=True, remove_columns=["text"])

    tokenized_train_dataset.set_format("torch")
    tokenized_val_dataset.set_format("torch")
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)
    val_dataloader = DataLoader(tokenized_val_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

    # --- ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • ---
    print("2. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì¤‘...")
    model = ArtistX(model_name=MODEL_NAME).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ.")

    # --- í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ---
    print("3. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    best_val_loss = float('inf')

    for epoch in range(NUM_EPOCHS):
        model.train()
        train_progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch + 1} [Train]")
        for batch in train_progress_bar:
            optimizer.zero_grad()
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            output_logits = model(batch['input_ids'], batch['attention_mask'])
            loss = loss_fn(output_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1))
            loss.backward()
            optimizer.step()
            train_progress_bar.set_postfix(loss=loss.item())

        model.eval()
        val_loss = 0
        val_progress_bar = tqdm(val_dataloader, desc=f"Epoch {epoch + 1} [Val]")
        with torch.no_grad():
            for batch in val_progress_bar:
                batch = {k: v.to(DEVICE) for k, v in batch.items()}
                output_logits = model(batch['input_ids'], batch['attention_mask'])
                loss = loss_fn(output_logits.view(-input_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1)))
                val_loss += loss.item()
                val_progress_bar.set_postfix(loss=loss.item())
        avg_val_loss = val_loss / len(val_dataloader)

        print(f"Epoch {epoch + 1} | Validation Loss: {avg_val_loss:.4f}")

        if avg_val_loss < best_val_loss:
            print(f"Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving model...")
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "artist_x_best_model.pth")

    print("\nâœ… í•™ìŠµì´ ìµœì¢… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ê³ ìœ í•œ ë§ˆìŠ¤í„° í‚¤ê°€ 'artist_x_best_model.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
