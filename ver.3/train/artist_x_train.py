# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ ---
import torch
from torch import nn # PyTorchì˜ ì‹ ê²½ë§ ëª¨ë“ˆ
from torch.utils.data import DataLoader # ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ìœ í‹¸ë¦¬í‹°
from transformers import BertModel, BertLMHeadModel, BertTokenizer, DataCollatorWithPadding # Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬
from datasets import load_dataset # Hugging Faceì˜ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from tqdm import tqdm # í•™ìŠµ ì§„í–‰ë¥ ì„ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import warnings
import os

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¼ë¶€ ê²½ê³  ë©”ì‹œì§€ëŠ” í•™ìŠµì— ì§€ì¥ì„ ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ ë•ë‹ˆë‹¤.
warnings.filterwarnings("ignore")

# --- 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ---
# ArtistX ëª¨ë¸ì˜ ì„¤ê³„ë„ì…ë‹ˆë‹¤.
class ArtistX(nn.Module):
    # ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__() # nn.Moduleì˜ ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ ë¨¼ì € í˜¸ì¶œ
        
        # í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ, í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í† í¬ë‚˜ì´ì €
        self.tokenizer = BertTokenizer.from_pretrained(model_name, local_files_only=True)
        # í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¸ì½”ë” ë¶€ë¶„ (BERTì˜ ê¸°ë³¸ ëª¨ë¸)
        self.encoder = BertModel.from_pretrained(model_name, local_files_only=True)
        # ì˜ë¯¸ë¡œë¶€í„° ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë””ì½”ë” ë¶€ë¶„ (BERT ìƒì„± ëª¨ë¸)
        self.decoder = BertLMHeadModel.from_pretrained(model_name, is_decoder=True, local_files_only=True)

    # ëª¨ë¸ì— ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ, ì‹¤ì œë¡œ ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ” í•¨ìˆ˜
    def forward(self, input_ids, attention_mask):
        # 1. ì¸ì½”ë”ë¥¼ í†µí•´ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì ì¬ ë²¡í„°(ê³ ì°¨ì›ì˜ ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        latent_vector = encoder_outputs.last_hidden_state
        
        # 2. ì–»ì–´ì§„ ì ì¬ ë²¡í„°ë¥¼ ë””ì½”ë”ì— ì…ë ¥í•˜ì—¬ ì›ë˜ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        #    ê²°ê³¼ëŠ” ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ë§ˆë‹¤ ì–´ë–¤ ë‹¨ì–´ê°€ ì˜¬ì§€ ì˜ˆì¸¡í•œ í™•ë¥ (logits)ì…ë‹ˆë‹¤.
        output_logits = self.decoder(inputs_embeds=latent_vector, attention_mask=attention_mask).logits
        
        return output_logits

    # í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ë³µì›ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‚¬ìš©ììš© í•¨ìˆ˜
    def reconstruct(self, text: str) -> str:
        self.eval() # ëª¨ë¸ì„ 'í‰ê°€ ëª¨ë“œ'ë¡œ ì „í™˜ (Dropout ë“±ì„ ë¹„í™œì„±í™”)
        with torch.no_grad(): # 'ê¸°ìš¸ê¸° ê³„ì‚°'ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì„
            # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³  ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ë””ë°”ì´ìŠ¤(GPU/CPU)ë¡œ ì´ë™
            inputs = self.tokenizer(text, return_tensors='pt', max_length=128, padding='max_length', truncation=True).to(self.encoder.device)
            
            # ëª¨ë¸ì„ í†µí•´ ë¡œì§“(logits)ì„ ì–»ìŒ
            logits = self.forward(inputs['input_ids'], inputs['attention_mask'])
            
            # ë¡œì§“ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í° IDë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ
            predicted_ids = torch.argmax(logits, dim=-1)
            
            # ì˜ˆì¸¡ëœ í† í° IDë“¤ì„ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            reconstructed_text = self.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
            return reconstructed_text

# --- 2. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•©ë‹ˆë‹¤.
if __name__ == '__main__':
    # --- í•™ìŠµì— í•„ìš”í•œ ì£¼ìš” ì„¤ì •ê°’ (í•˜ì´í¼íŒŒë¼ë¯¸í„°) ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu" # GPUê°€ ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ cpu ì‚¬ìš©
    MODEL_NAME = 'bert-base-uncased' # ì‚¬ìš©í•  ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì´ë¦„
    LEARNING_RATE = 2e-5 # í•™ìŠµë¥ : ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ í° ë³´í­ìœ¼ë¡œ ì •ë‹µì„ ì°¾ì•„ê°ˆì§€ ê²°ì •
    BATCH_SIZE = 16    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ: í•œ ë²ˆì— ëª‡ ê°œì˜ ë°ì´í„°ë¥¼ ë³´ê³  í•™ìŠµí• ì§€ ê²°ì •
    NUM_EPOCHS = 20    # ì—í¬í¬: ì „ì²´ ë°ì´í„°ì…‹ì„ ì´ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ ê²°ì •

    print(f"ğŸš€ ArtistX í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. Device: {DEVICE}")

    # --- ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ ---
    print("1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    raw_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1') # Hugging Face Hubì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    
    # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í° IDë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def tokenize_function(examples):
        # í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³ , ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ìë¥´ê³ , ëª¨ìë¼ë©´ ì±„ì›ë‹ˆë‹¤.
        return tokenizer(examples["text"], truncation=True, max_length=128)

    # .map()ì„ ì‚¬ìš©í•´ ë°ì´í„°ì…‹ ì „ì²´ì— í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ ì ìš©
    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_dataset.set_format("torch") # ë°ì´í„°ì…‹ í˜•ì‹ì„ PyTorch í…ì„œë¡œ ì§€ì •

    # Data Collator: ë°°ì¹˜ ë‚´ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì— ë§ì¶° ë™ì ìœ¼ë¡œ íŒ¨ë”©ì„ ì¶”ê°€í•´ì£¼ëŠ” ì—­í• 
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # DataLoader: ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• 
    train_dataloader = DataLoader(tokenized_dataset["train"], shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)
    val_dataloader = DataLoader(tokenized_dataset["validation"], batch_size=BATCH_SIZE, collate_fn=data_collator)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

    # --- ëª¨ë¸ ë° í•™ìŠµ ë„êµ¬ ì„¤ì • ---
    print("2. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì¤‘...")
    model = ArtistX(model_name=MODEL_NAME).to(DEVICE) # ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ë³´ëƒ„
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) # ì˜µí‹°ë§ˆì´ì €(ìµœì í™” ë„êµ¬) ì„¤ì •
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (íŒ¨ë”© í† í°ì€ ë¬´ì‹œ)
    print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ.")

    # --- ë³¸ê²©ì ì¸ í•™ìŠµ ë£¨í”„ ---
    print("3. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    best_val_loss = float('inf') # ê°€ì¥ ë‚®ì€ ê²€ì¦ ì†ì‹¤ ê°’ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë³€ìˆ˜. ë¬´í•œëŒ€ë¡œ ì´ˆê¸°í™”.

    # ì •í•´ì§„ ì—í¬í¬ ìˆ˜ë§Œí¼ ì „ì²´ ë°ì´í„°ì…‹ì„ ë°˜ë³µ
    for epoch in range(NUM_EPOCHS):
        print(f"\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---")
        
        # -- í›ˆë ¨(Training) ë‹¨ê³„ --
        model.train() # ëª¨ë¸ì„ 'í›ˆë ¨ ëª¨ë“œ'ë¡œ ì „í™˜
        train_loss = 0
        train_progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch + 1} [Train]")
        for batch in train_progress_bar:
            optimizer.zero_grad() # ì´ì „ ë°°ì¹˜ì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ ì´ˆê¸°í™”
            batch = {k: v.to(DEVICE) for k, v in batch.items()} # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            output_logits = model(batch['input_ids'], batch['attention_mask']) # ëª¨ë¸ ì˜ˆì¸¡
            loss = loss_fn(output_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1)) # ì†ì‹¤ ê³„ì‚°
            loss.backward() # ì—­ì „íŒŒ: ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê° íŒŒë¼ë¯¸í„°ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°
            optimizer.step() # ì˜µí‹°ë§ˆì´ì €ê°€ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
            train_loss += loss.item() # í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤ ê°’ì„ ëˆ„ì 
            train_progress_bar.set_postfix(loss=loss.item()) # ì§„í–‰ë¥  í‘œì‹œì¤„ì— í˜„ì¬ ì†ì‹¤ ê°’ í‘œì‹œ
        avg_train_loss = train_loss / len(train_dataloader)

        # -- ê²€ì¦(Validation) ë‹¨ê³„ --
        model.eval() # ëª¨ë¸ì„ 'í‰ê°€ ëª¨ë“œ'ë¡œ ì „í™˜
        val_loss = 0
        val_progress_bar = tqdm(val_dataloader, desc=f"Epoch {epoch + 1} [Val]")
        with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”
            for batch in val_progress_bar:
                batch = {k: v.to(DEVICE) for k, v in batch.items()}
                output_logits = model(batch['input_ids'], batch['attention_mask'])
                loss = loss_fn(output_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1))
                val_loss += loss.item()
                val_progress_bar.set_postfix(loss=loss.item())
        avg_val_loss = val_loss / len(val_dataloader)

        print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}")

        # -- ìµœì  ëª¨ë¸ ì €ì¥ --
        # í˜„ì¬ ê²€ì¦ ì†ì‹¤ì´ ì´ì „ì— ê¸°ë¡ëœ ìµœì € ì†ì‹¤ë³´ë‹¤ ë‚®ìœ¼ë©´, ëª¨ë¸ì„ ì €ì¥
        if avg_val_loss < best_val_loss:
            print(f"Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving model...")
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "artist_x_best_model.pth")

    print("\nâœ… í•™ìŠµì´ ìµœì¢… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ìµœì  ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ 'artist_x_best_model.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")