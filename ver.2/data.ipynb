{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33971237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 253488.15 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 942137.18 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 429341.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 'wikitext-2-raw-v1'은 적당한 크기의 버전으로, 테스트하기에 좋습니다.\n",
    "raw_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# 다운로드된 데이터셋 구조 확인\n",
    "print(raw_dataset)\n",
    "# 출력 결과:\n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['text']   num_rows: 36718\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['text'],\n",
    "#         num_rows: 360\n",
    "#     })\n",
    "#     test: Dataset({\n",
    "#         features: ['text'],\n",
    "#         num_rows: 4358\n",
    "#     })\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028c57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징을 시작합니다. (데이터 양에 따라 몇 분 정도 걸릴 수 있습니다)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:03<00:00, 1297.82 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:25<00:00, 1424.09 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:02<00:00, 1368.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이징 완료!\n",
      "\n",
      "--- 전처리 후 데이터셋 ---\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 모델과 동일한 토크나이저 준비\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    # 텍스트를 토큰 ID, 어텐션 마스크 등으로 변환\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", # 문장 길이를 맞춰주기 위해 패딩\n",
    "        truncation=True,      # 최대 길이를 넘는 부분은 잘라냄\n",
    "        max_length=128        # 문장의 최대 길이를 128 토큰으로 설정\n",
    "    )\n",
    "\n",
    "# .map()을 사용해 전체 데이터셋에 함수를 적용\n",
    "print(\"토크나이징을 시작합니다. (데이터 양에 따라 몇 분 정도 걸릴 수 있습니다)...\")\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "print(\"✅ 토크나이징 완료!\")\n",
    "\n",
    "# 전처리 후 결과 확인\n",
    "print(\"\\n--- 전처리 후 데이터셋 ---\")\n",
    "print(tokenized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
