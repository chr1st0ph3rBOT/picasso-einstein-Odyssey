import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import BertModel, BertLMHeadModel, BertTokenizer, DataCollatorWithPadding
from datasets import load_dataset
from tqdm import tqdm  # tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import warnings

warnings.filterwarnings("ignore")

## 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class ArtistX(nn.Module):
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.encoder = BertModel.from_pretrained(model_name)
        # ğŸ’¡ FIX: is_decoder=True ì˜µì…˜ ì¶”ê°€
        self.decoder = BertLMHeadModel.from_pretrained(model_name, is_decoder=True)

    def forward(self, input_ids, attention_mask):
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        latent_vector = encoder_outputs.last_hidden_state
        output_logits = self.decoder(inputs_embeds=latent_vector, attention_mask=attention_mask).logits
        return output_logits

    def reconstruct(self, text: str) -> str:
        self.eval()
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors='pt', max_length=128, padding='max_length', truncation=True).to(self.encoder.device)
            logits = self.forward(inputs['input_ids'], inputs['attention_mask'])
            predicted_ids = torch.argmax(logits, dim=-1)
            reconstructed_text = self.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
            return reconstructed_text

## 2. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
if __name__ == '__main__':
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MODEL_NAME = 'bert-base-uncased'
    LEARNING_RATE = 5e-5
    BATCH_SIZE = 16
    NUM_EPOCHS = 10

    print(f"ğŸš€ ArtistX í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. Device: {DEVICE}")

    # --- ë°ì´í„° ì¤€ë¹„ ---
    print("1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    # ğŸ’¡ FIX: trust_remote_code=True ì‚­ì œ
    raw_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')
    
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=128)

    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_dataset.set_format("torch")
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    train_dataloader = DataLoader(tokenized_dataset["train"], shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

    # --- ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • ---
    print("2. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì¤‘...")
    model = ArtistX(model_name=MODEL_NAME).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ.")

    # --- í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ---
    print("3. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model.train()
    for epoch in range(NUM_EPOCHS):
        print(f"\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---")
        # ğŸ’¡ LOGGING: tqdmì„ ì‚¬ìš©í•´ ì§„í–‰ë¥  í‘œì‹œì¤„ ìƒì„±
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch + 1}")
        
        for batch in progress_bar:
            optimizer.zero_grad()
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            output_logits = model(batch['input_ids'], batch['attention_mask'])
            loss = loss_fn(output_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1))
            loss.backward()
            optimizer.step()
            
            # ğŸ’¡ LOGGING: ì§„í–‰ë¥  í‘œì‹œì¤„ì— ì‹¤ì‹œê°„ ì†ì‹¤ ê°’ ì—…ë°ì´íŠ¸
            progress_bar.set_postfix(loss=loss.item())

    print("\nâœ… í•™ìŠµì´ ìµœì¢… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    torch.save(model.state_dict(), "artist_x_weights.pth")
    print("ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ 'artist_x_weights.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")