import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import BertModel, BertForCausalLM, BertTokenizer, DataCollatorWithPadding
from datasets import load_dataset
import warnings

# ê²½ê³  ë©”ì‹œì§€ ë„ê¸° (ì„ íƒ ì‚¬í•­)
warnings.filterwarnings("ignore")

# 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class ArtistX(nn.Module):
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.encoder = BertModel.from_pretrained(model_name)
        self.decoder = BertForCausalLM.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask):
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        latent_vector = encoder_outputs.last_hidden_state
        output_logits = self.decoder(inputs_embeds=latent_vector, attention_mask=attention_mask).logits
        return output_logits

    def reconstruct(self, text: str) -> str:
        self.eval()
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors='pt', max_length=128, padding='max_length', truncation=True).to(self.encoder.device)
            logits = self.forward(inputs['input_ids'], inputs['attention_mask'])
            predicted_ids = torch.argmax(logits, dim=-1)
            reconstructed_text = self.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
            return reconstructed_text

# 2. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
if __name__ == '__main__':
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MODEL_NAME = 'bert-base-uncased'
    LEARNING_RATE = 5e-5
    BATCH_SIZE = 16 # GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ 8 ë˜ëŠ” 4ë¡œ ì¤„ì´ì„¸ìš”.
    NUM_EPOCHS = 3

    print(f"ğŸš€ ArtistX í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. Device: {DEVICE}")

    # --- ë°ì´í„° ì¤€ë¹„ ---
    print("1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    raw_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', trust_remote_code=True)

    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=128)

    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_dataset.set_format("torch")
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    train_dataloader = DataLoader(tokenized_dataset["train"], shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

    # --- ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • ---
    print("2. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì¤‘...")
    model = ArtistX(model_name=MODEL_NAME).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ.")

    # --- í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ---
    print("3. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model.train()
    for epoch in range(NUM_EPOCHS):
        print(f"\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---")
        for i, batch in enumerate(train_dataloader):
            if i % 500 == 0 and i > 0:
                print(f"  Batch {i}/{len(train_dataloader)}")
            optimizer.zero_grad()
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            output_logits = model(batch['input_ids'], batch['attention_mask'])
            loss = loss_fn(output_logits.view(-1, tokenizer.vocab_size), batch['input_ids'].view(-1))
            loss.backward()
            optimizer.step()

    print("\nâœ… í•™ìŠµì´ ìµœì¢… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    torch.save(model.state_dict(), "artist_x_weights.pth")
    print("ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ 'artist_x_weights.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")