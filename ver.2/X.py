import torch
from torch import nn
from transformers import BertModel, BertForCausalLM, BertTokenizer

# ---------------------------------
# 1. ëª¨ë¸ ì •ì˜ (ArtistX)
# ---------------------------------
class ArtistX(nn.Module):
    """
    ArtistX: í…ìŠ¤íŠ¸ë¥¼ ì ì¬ ë²¡í„°ë¡œ ì¸ì½”ë”©í•˜ê³ , ë‹¤ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì™„ë²½í•˜ê²Œ ë³µì›í•©ë‹ˆë‹¤.
    (Text -> Latent Vector -> Original Text)
    """
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        
        # Xì˜ ì¸ì½”ë”: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë¶€ë¶„
        self.encoder = BertModel.from_pretrained(model_name)
        
        # Xì˜ ë””ì½”ë”: ì˜ë¯¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ë¶€ë¶„
        # CausalLMì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆì–´ í…ìŠ¤íŠ¸ ìƒì„±ì— ì í•©í•©ë‹ˆë‹¤.
        self.decoder = BertForCausalLM.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask):
        """ëª¨ë¸ì˜ í•µì‹¬ ë¡œì§: ì¸ì½”ë”© -> ë””ì½”ë”©"""
        
        # 1. ì¸ì½”ë”ë¥¼ í†µí•´ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì ì¬ ë²¡í„°(ì˜ë¯¸)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        #    outputs.last_hidden_stateì˜ shape: [batch_size, sequence_length, hidden_size]
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        latent_vector = encoder_outputs.last_hidden_state
        
        # 2. ì–»ì–´ì§„ ì ì¬ ë²¡í„°ë¥¼ ë””ì½”ë”ì— ì…ë ¥í•˜ì—¬ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë³µì›í•˜ë ¤ ì‹œë„í•©ë‹ˆë‹¤.
        #    ë””ì½”ë”ëŠ” ì´ ì ì¬ ë²¡í„°ë¥¼ ë³´ê³  ì›ë³¸ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        #    logitsì˜ shape: [batch_size, sequence_length, vocab_size]
        output_logits = self.decoder(inputs_embeds=latent_vector, attention_mask=attention_mask).logits
        
        return output_logits

    def reconstruct(self, text: str) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì ì¸ ë³µì› í•¨ìˆ˜"""
        self.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors='pt')
            logits = self.forward(inputs['input_ids'], inputs['attention_mask'])
            
            # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í° IDë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ
            predicted_ids = torch.argmax(logits, dim=-1)
            
            # ì˜ˆì¸¡ëœ í† í° IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            reconstructed_text = self.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
            return reconstructed_text

# ---------------------------------
# 2. X ëª¨ë¸ í•™ìŠµ (ê°œë… ì½”ë“œ)
# ---------------------------------
def train_artist_x_conceptual():
    """
    ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ ì‹¤í–‰ë˜ì§€ ì•Šìœ¼ë©°, Xë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê³¼ì •ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ê°œë…ì ì¸ ì½”ë“œì…ë‹ˆë‹¤.
    ì‹¤ì œ í•™ìŠµì—ëŠ” ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ê³ ì‚¬ì–‘ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """
    print("--- ArtistX í•™ìŠµ ê³¼ì • (ê°œë…) ---")
    
    # 1. ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
    model_X = ArtistX()
    optimizer = torch.optim.Adam(model_X.parameters(), lr=1e-5)
    loss_fn = nn.CrossEntropyLoss()
    
    # 2. ğŸ“š ë°ì´í„° ë¡œë” ì¤€ë¹„ (ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ë¼ê³  ê°€ì •)
    # text_dataset = load_your_massive_text_dataset()
    # data_loader = DataLoader(text_dataset, batch_size=16)

    # 3. í•™ìŠµ ë£¨í”„
    # for epoch in range(num_epochs):
    #     for batch in data_loader:
    #         optimizer.zero_grad()
    #         input_ids, attention_mask = batch['input_ids'], batch['attention_mask']
            
    #         output_logits = model_X(input_ids, attention_mask)
            
    #         # ğŸ’¡ ëª©í‘œ: ëª¨ë¸ì˜ ì˜ˆì¸¡(output_logits)ì´ ì›ë³¸(input_ids)ê³¼ ê°™ì•„ì§€ë„ë¡ í•™ìŠµ
    #         loss = loss_fn(output_logits.view(-1, model_X.tokenizer.vocab_size), input_ids.view(-1))
            
    #         loss.backward()
    #         optimizer.step()
    #     print(f"Epoch {epoch+1} ì™„ë£Œ, Loss: {loss.item()}")

    # 4. í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
    # torch.save(model_X.state_dict(), "artist_x_weights.pth")
    print("í•™ìŠµì´ ì™„ë£Œë˜ê³  'artist_x_weights.pth' íŒŒì¼ì´ ì €ì¥ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.")

# ---------------------------------
# 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ---------------------------------
if __name__ == '__main__':
    print("ğŸ¨ 'ê¸°ì–µí•˜ëŠ” í™”ê°€' ArtistXë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    # ê°œë…ì ì¸ í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
    train_artist_x_conceptual()
    
    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    X = ArtistX()
    
    # ğŸ’¡ ì¤‘ìš”: ì‹¤ì œë¡œëŠ” ì•„ë˜ ì£¼ì„ì„ í’€ê³  í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.
    # X.load_state_dict(torch.load("artist_x_weights.pth"))
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥
    original_text = "a painter who remembers what he drew"
    
    # Xì˜ ë³µì› ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    reconstructed_text = X.reconstruct(original_text)
    
    print("\n--- Xì˜ ì¬êµ¬ì„± ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ ---")
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {original_text}")
    print(f"ë³µì›ëœ í…ìŠ¤íŠ¸: {reconstructed_text}")
    print("\n(ì°¸ê³ : í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì´ë¯€ë¡œ, í˜„ì¬ ì¶œë ¥ì€ ì˜ë¯¸ ì—†ëŠ” ë¬´ì‘ìœ„ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.)")